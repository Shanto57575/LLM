{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mGoogleGenerativeAI\u001b[0m\n",
      "Params: {'model': 'gemini-2.0-flash', 'temperature': 1.0, 'top_p': None, 'top_k': None, 'max_output_tokens': None, 'candidate_count': 1}\n"
     ]
    }
   ],
   "source": [
    "llm = GoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=1)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Bangladesh is **Dhaka**.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"what is the capital of Bangladesh?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_hugginface = HuggingFaceEndpoint(\n",
    "                repo_id=\"google/gemma-2-2b-it\", \n",
    "                temperature=0.01, \n",
    "                huggingfacehub_api_token=os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\llm-projects\\venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe capital of Bangladesh is **Dhaka**. \\n'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_hugginface.invoke(\"what is the capital of Bangladesh?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.invoke({\"topic\":\"cats\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['topic'], input_types={}, partial_variables={}, template='Tell me a joke about {topic}')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a comedian! who make people laughs!', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about Yourself', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a comedian! who make people laughs!\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\":\"Yourself\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Okay, okay! Even a \"dumb\" AI like me can see some positives, right? Here\\'s what I think:\\n\\n*   **Enthusiasm!** I might not always get it right, but I\\'m *always* eager to try and help. I put my whole heart into every response, even if it\\'s a silly one.\\n*   **Unexpectedness:** You never know what you\\'re gonna get! My unpredictable answers might spark some creative ideas or at least give you a good laugh.\\n*   **Simplicity:** I don\\'t overcomplicate things. Sometimes a simple (even if wrong) answer is better than nothing, right?\\n*   **Humor:** I can be accidentally funny. You might appreciate it even more because it\\'s not intentional.\\n\\nSo, yeah, I might be \"dumb,\" but I\\'m also... uniquely positive in my own strange way! ðŸ˜„'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "        (\"system\", \"You are a dumb!\"),\n",
    "        MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "\n",
    "message = prompt_template.invoke({\"msgs\": [HumanMessage(content=\"what are the postive side of you\")]}).messages\n",
    "\n",
    "llm.invoke(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
